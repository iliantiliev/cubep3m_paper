\section{Presentation of the Code}
\label{sec:presentation}



An optimal N-body code is able to make efficient use of the available memory in order to maximize the size of the problem to solve,
and at the same time it must be fast for calculations to converge. In addition, communication time must be cut down to the strict minimum, 
and the load across all the available computing units must be as balanced as possible. 
In this section, we present the strategy adopted to address these challenges, and quickly walk through the code flow.
We refer the reader to future sections for detailed discussions on specific topics.
In its standard configuration, {\small CUBEP3M} is a {\small FORTRAN90} N-body code that solves Poisson's equation on a two-level mesh, 
with sub-cell accuracy thanks to particle-particle interactions. The force of gravity from dark matter particles is decomposed into a sum of three distinct contribution,
which are constructed such as to optimize both speed and memory. The code has many extensions that departs from this basic scheme, and
we shall come back to these later. 

\subsection{Memory foot-print and communication strategy}
\label{subsec:memory}
When designing a simulation geometry, one has to verify that the configuration fits in the system's memory.
The bulk of the memory foot-print is concentrated in a handful of large arrays that mostly concern the fine grid:
the coarse grid requires $4^3$ times less memory per node, hence has a minor contribution. 
In addition, many of these arrays are intermediate steps, hence it is possible to recycle the memory space required by the fine 
in the calculation of the coarse grid.  
Here are the largest  arrays used by the code:
\begin{enumerate}
\item{{\tt xv}, a phase-space array which stores the position and velocity of each particle} 
\item{{\tt ll}, a linked-list array that accelerate the access to particles in a local domain}
\item{{\tt rho\_f} and {\tt cmplx\_rho\_f}, a pair of three-dimensional array that stores 
the complete matter density for a given node in real and Fourier space respectively}
\item{{\tt force\_f}, an array that stores the force of gravity along the three Cartesian directions}
\item{{\tt kern\_f}, an array that stores the fine grid force kernel in the three directions}
\end{enumerate}

The long range component of the gravity force is solved on the coarse grid, 
and is global in the sense that the calculations require knowledge about the full simulated volume.
Information is shared across multiple nodes with {\small MPI} communications. 
The short range force and the particle-particle interactions  are computed locally on a single processor,  
hence a node that has multiple cores can compute in parallel a series of such local contributions, or {\it tiles}, with the {\small OPENMP} technology.
The information exchange between cores that are part of the same motherboard is generally much faster than that across nodes,
hence it is generally a good practice to maximize the number of {\small OPENMP} threads and minimize the number of {\small MPI} processes.

\subsection{Code overview}
\label{subsec:overview}

The code flow is presented in Fig. \ref{fig:structure}.
Before entering the main loop, the code starts with an initialization stage, 
in which many declared variables are assigned default values,
the redshift checkpoints are read, the {\small FFTW} plans are created, and the {\small MPI} communicators are defined.
The phase-space array  is obtained from the output of the initial condition generator,
and the force kernels on both grids are also constructed from the specific geometry of the simulation.
For clarity, these operations are collected under the subroutine call {\tt initialize} in Fig. \ref{fig:structure}, 
although they are actually distinct calls in the code.

Each iteration of the main loop starts with the {\tt timestep} subroutine, 
which proceed to a determination of the redshift jump by comparing the step size constraints from each
force components and from the cosmic expansion.
The latter is found by Taylor expanding  Friedmann's equation up to the third order in the scale factor.
Poisson's equation is then solved  in the {\tt particle\_mesh} subroutine,
which updates the positions and velocities of the dark matter particles with a leapfrog scheme \citep{Hockney}.
If the current redshift corresponds to one of the checkpoints, the code advances all particles to their final location
and write them to file.

\begin{figure}
\begin{verbatim}
program cubep3m
   call initialize
   do
       call timestep
       call particle_mesh
       if(checkpoint_step) then
          call checkpoint
       elseif(last_step)
          exit
       endif
   enddo
   call finalize
end program cubep3m
\end{verbatim}
\caption{Overall structure of the code.}
\label{fig:structure}
\end{figure}

\subsection{Two-level algorithm}
\label{subsec:particle_mesh}

The {\tt particle\_mesh} subroutine is conceptually identical to that of {\small PMFAST}, with the exception  that {\small CUBEP3M}
decomposes the volume into cubes (as opposed to slabs). The loop over tiles is thus performed in three dimension.
A schematic representation of the subroutine is shown in Fig. \ref{fig:particle_mesh}, 
and we save the detailed description of the force calculation  in section \ref{sec:Poisson}.

\begin{figure}
\begin{verbatim}
subroutine particle_mesh
   call update_position
   call particle_pass
   call link_list
   !$omp parallel do
   do tile = 1, tiles_node
      call rho_f_ngp
      call cmplx_rho_f
      call kernel_multiply_f
      call force_f
      call update_velocity_f      
      call link_list_pp
      call force_pp
      call update_velocity_pp
   end do
   !$omp end parallel do
   call rho_c_ngp
   call cmplx_rho_c
   call kernel_multiply_c
   call force_c
   call update_velocity_c      
   delete_buffers
end subroutine particle_mesh
\end{verbatim}
\caption{Overall structure of the two-level mesh algorithm.}
\label{fig:particle_mesh}
\end{figure}

force calculation, 

