\section{Introduction}

Many physical and astrophysical systems are subject to non-linear dynamics
and rely on N-body simulations to describe the evolution of bodies. 
One of the main field of application is the modelling of large scale structures, 
which are driven by the sole force of gravity. Recent observations of the cosmic microwave background
\citep{WMAP5-7} of galaxy clustering \citep{2df, SDSS, WiggleZ, BOSS} of weak gravitational lensing \citep{CFHTLS, SDSS}
and of supernovae redshift-distance relations all point towards a standard model of cosmology, in which dark energy
and collisionless dark matter occupy more than 95 per cent of the total energy density of the universe.
In such a paradigm, pure N-body code are perfectly suited to describe the dynamics, as long as we
understands where and how the baryonic fluid feeds back on the dark matter structure.
The next generation of measurements aim at constraining the cosmological parameters at the per cent level, 
and the theoretical understanding of the non-linear dynamics that govern structure formation heavily relies 
on numerical simulations. 

For instance, a measurement of the baryonic acoustic oscillation (BAO) dilation scale can provide
tight constraints on the dark energy equation of scale \citep{Seo2003,2005}.
The most optimal estimates of the uncertainty requires the knowledge of the 
matter power spectrum covariance matrix, which is only accurate when measured from a large sample 
of N-body simulations \citep{RH2005, japan2,japan3, Ngan}.
For the same reasons, the most accurate estimates of weak gravitational lensing signal is obtained
by propagating photons in past light cones that are extracted from simulated density fields \citep{HarnoisLudo, Takahashi }.
{\bf (Ilian, could you say something about reionization here?)}

The basic problem that is addressed with N-body codes is a time evolution of an ensemble of $N$ particles
that is subject to gravitational attraction. The brute force calculation requires $O(N^{2})$ operation, a cost that 
exceeds the memory and speed of current machines for large problems.
Solving the problem  on a mesh \citep{Hockney} reduces to $O(N\mbox{log}N)$ the number of operations,
as it is possible to solve for the particle-mesh (PM) interaction with fast Fourier transforms techniques, 
typically using high performance libraries such as {\small FFTW} \citep{FFTW}.


With the advent of large computing facilities, parallel coding has now become common practice, 
and N-body codes have evolved both in performance and complexity. 
Many codes have opted for a tree algorithm \citep{Gadjet, Gadjet2, TPM, GOTPM}, in which 
the local resolution increases with the density of the matter field. 
These have the advantage to balance the load across the computing units, which enable the calculation of high density regions. 
The drawback is a significant loss in speed, which can be only partly recovered by turning off the tree algorithm. 
Unlike Hydra \citep{couchman1991} and the PM code by \cite{FerrelBertschinger1995},
 these are not designed to perform large scale PM calculations. 

{\small PMFAST} (\cite{PMFAST}, MPT hereafter) is one of the first code that is designed such as to optimize the PM algorithm,
both in terms of speed and memory usage. It uses a two-level mesh algorithm based on the gravity solver of \cite{TracPen2003},
The long range gravitational force is computed on a  grid four times coarser, such as to minimize the communication time
and to fit in system's memory. The short range is computed locally on a finer mesh, which can be as tight as the memory allows.
This enable the code to evolve very large cosmological systems both rapidly and accurately, on relatively modest clusters.




{\bf (Describe briefly   Hydra, PM code by japanese, and others...)}








